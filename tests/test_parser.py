"""
Test Script for Document Parser Components

This script tests the document processing pipeline components individually
to validate their efficacy for PDF text and data extraction.
"""

import sys
from pathlib import Path

# Add project root to path for imports
sys.path.append(str(Path(__file__).parent.parent))

from document_processor import DocumentExtractor, DocumentClassifier, EntityParser


def test_document_extractor(file_path):
    """Test the DocumentExtractor component."""
    print(f"\nğŸ” TESTING DOCUMENT EXTRACTOR")
    print(f"File: {file_path}")
    print("-" * 50)
    
    try:
        extractor = DocumentExtractor()
        result = extractor.extract_text(file_path)
        
        text = result['text']
        metadata = result['metadata']
        
        print(f"âœ… Extraction successful!")
        print(f"ğŸ“„ Pages: {metadata.get('pages', 'N/A')}")
        print(f"ğŸ”§ Method: {metadata.get('extraction_method', 'N/A')}")
        print(f"ğŸ“ Text length: {len(text)} characters")
        print(f"ğŸ“ Word count: {len(text.split())} words")
        
        # Show extraction issues if any
        extraction_issues = metadata.get('extraction_issues', [])
        if extraction_issues:
            print(f"âš ï¸  Extraction Issues: {len(extraction_issues)}")
            for issue in extraction_issues[:3]:  # Show first 3 issues
                print(f"   â€¢ {issue}")
        
        # Show first 500 characters
        print(f"\nğŸ“– EXTRACTED TEXT PREVIEW:")
        print("-" * 30)
        print(text[:500] + "..." if len(text) > 500 else text)
        
        return result
        
    except Exception as e:
        print(f"âŒ Extraction failed: {str(e)}")
        return None


def test_document_classifier(extracted_result):
    """Test the DocumentClassifier component."""
    print(f"\nğŸ·ï¸  TESTING DOCUMENT CLASSIFIER")
    print("-" * 50)
    
    if not extracted_result:
        print("âŒ No extracted text to classify")
        return None
    
    try:
        classifier = DocumentClassifier()
        text = extracted_result['text']
        metadata = extracted_result['metadata']
        
        classification = classifier.classify_document(text, metadata)
        
        doc_type = classification['document_type'].value
        confidence = classification['confidence']
        matched_patterns = classification['matched_patterns']
        reason = classification['classification_reason']
        
        print(f"âœ… Classification successful!")
        print(f"ğŸ“‹ Document Type: {doc_type}")
        print(f"ğŸ¯ Confidence: {confidence:.2f}")
        print(f"ğŸ” Matched Patterns: {len(matched_patterns)}")
        print(f"ğŸ’­ Reason: {reason}")
        
        if matched_patterns:
            print(f"\nğŸ¯ MATCHED PATTERNS:")
            for pattern in matched_patterns:
                print(f"   â€¢ {pattern}")
        
        return classification
        
    except Exception as e:
        print(f"âŒ Classification failed: {str(e)}")
        return None


def test_entity_parser(extracted_result):
    """Test the EntityParser component."""
    print(f"\nğŸ” TESTING ENTITY PARSER")
    print("-" * 50)
    
    if not extracted_result:
        print("âŒ No extracted text to parse")
        return None
    
    try:
        parser = EntityParser()
        text = extracted_result['text']
        
        entities = parser.parse_entities(text)
        
        print(f"âœ… Entity extraction successful!")
        
        # Display summary
        summary = entities.get('summary', {})
        print(f"\nğŸ“Š ENTITY SUMMARY:")
        for key, count in summary.items():
            print(f"   {key}: {count}")
        
        # Display specific entities
        print(f"\nğŸ“‹ EXTRACTED ENTITIES:")
        
        # GSTIN Numbers
        gstin_numbers = entities.get('gstin_numbers', [])
        if gstin_numbers:
            print(f"   ğŸ¢ GSTIN Numbers: {gstin_numbers}")
        
        # Dates
        dates = entities.get('dates', [])
        if dates:
            print(f"   ğŸ“… Dates found: {len(dates)}")
            for date_info in dates[:3]:  # Show first 3
                print(f"      â€¢ {date_info['original']} â†’ {date_info['normalized']}")
        
        # Amounts
        amounts = entities.get('amounts', [])
        if amounts:
            print(f"   ğŸ’° Amounts found: {len(amounts)}")
            for amount_info in amounts[:3]:  # Show first 3
                print(f"      â€¢ {amount_info['original']} â†’ {amount_info['cleaned']}")
        
        # Legal Sections
        sections = entities.get('legal_sections', [])
        if sections:
            print(f"   âš–ï¸  Legal Sections: {sections}")
        
        # Form Numbers
        forms = entities.get('form_numbers', [])
        if forms:
            print(f"   ğŸ“„ Form Numbers: {forms}")
        
        # Case Numbers
        cases = entities.get('case_numbers', [])
        if cases:
            print(f"   ğŸ“‹ Case Numbers: {cases}")
        
        return entities
        
    except Exception as e:
        print(f"âŒ Entity parsing failed: {str(e)}")
        return None


def test_case_folder(case_path):
    """Test all documents in a case input folder."""
    input_path = Path(case_path) / "input"
    
    if not input_path.exists():
        print(f"âš ï¸  Input folder not found: {input_path}")
        return
    
    # Get all PDF and text files in input folder
    input_files = list(input_path.glob("*.pdf")) + list(input_path.glob("*.txt"))
    
    if not input_files:
        print(f"âš ï¸  No input documents found in: {input_path}")
        return
    
    print(f"\nğŸ§ª TESTING CASE: {case_path}")
    print(f"ğŸ“ Input documents: {len(input_files)}")
    print("=" * 60)
    
    for doc_path in input_files:
        print(f"\nğŸ“„ Processing: {doc_path.name}")
        print("-" * 40)
        
        # Test 1: Document Extraction
        extracted_result = test_document_extractor(str(doc_path))
        
        # Test 2: Document Classification
        classification_result = test_document_classifier(extracted_result)
        
        # Test 3: Entity Parsing
        entity_result = test_entity_parser(extracted_result)


def main():
    """Main test function."""
    print("ğŸš€ GST Law Co-pilot - Document Parser Testing")
    print("=" * 60)
    
    # Check for organized case structure first
    affidavits_path = Path("data/affidavits")
    if affidavits_path.exists():
        case_folders = [d for d in affidavits_path.iterdir() if d.is_dir()]
        
        if case_folders:
            print(f"ğŸ“ Found {len(case_folders)} case folder(s) in data/affidavits/")
            for case_folder in sorted(case_folders):
                test_case_folder(case_folder)
        else:
            print("ğŸ“ No case folders found in data/affidavits/")
        
    print(f"\nâœ… Testing complete!")
    print(f"ğŸ’¡ Review the results above to assess parser efficacy")
    print(f"ğŸ“‹ To test with organized cases, add folders to data/affidavits/ with input/ subdirectories")


if __name__ == "__main__":
    main()
